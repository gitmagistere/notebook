{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Didactique de l'informatique\n",
    "\n",
    "## Compétence EVALUER\n",
    "\n",
    "* Capacité à attribuer mentalement une valeur (résultat, type...) à un programme donné. \n",
    "\n",
    "L'évaluation d'un programme - lui donner une valeur - peut se faire de différentes manières dans différents domaines. Le plus simple est bien sûr d'évaluer le résultat que donne le programme à l'exécution. Mais il est aussi utile de savoir évaluer le **type** du résultat sans chercher nécessairement à connaître sa valeur.\n",
    "\n",
    "> It is interpreting code as data and data as code. It is type checking as the generalization of dimensional analysis. (Extrait de : \"Computational thinking, Jeannette Wing\").\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exemple** : Soit l'expression suivante prévue pour calculer si une année est bissextile :\n",
    "\n",
    "```an % 4==0 and (not an % 100 == 0 or an % 400 == 0)\n",
    "```\n",
    "\n",
    "On peut au préalable calculer le type du résultat de cette expression en fonction du type des arguments. On sait que `==` prend deux arguments de type quelconque et donne un résultat de type `bool`, que `or` et `and` donne des résultats de type `bool` donc l'expression complète donne un résultat de type `bool`. On peut vérifier :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "an = 2019\n",
    "type(an % 4==0 and (not an % 100 == 0 or an % 400 == 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluer** c'est aussi donner la valeur du résultat de l'exécution - ce que fournit l'interprète."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "an % 4==0 and (not an % 100 == 0 or an % 400 == 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* L'évaluation mentale peut être concrète. On peut construire à la main un tableau d'exécution du programme en notant sur chaque ligne les valeurs prises par les variables à certains points d'observation du programme :\n",
    "\n",
    "**Exemple** : on évalue le résultat du programme suivant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EvaluerPolynome(p,x):\n",
    "    y = 0 # Point d'observation 1\n",
    "    for i in range (len(p)):\n",
    "        y = y * x + p[i]  # Point d'observation 2\n",
    "    return(y)\n",
    "EvaluerPolynome([1,4,3], 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Point | p | i | x | y |\n",
    "|---|---|---|---|---|\n",
    "| Point 1 | [1,4,3] | - | 10 | 0 |\n",
    "| Point 2 | [1,4,3] | 0 | 10 | 1 |\n",
    "| Point 2 | [1,4,3] | 1 | 10 | 14 |\n",
    "| Point 2 | [1,4,3] | 2 | 10 | 143|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* On peut aussi **évaluer** symboliquement. Dans ce cas on évalue *à la main* le résultat en nommant les valeurs des paramètres : `EvaluerPolynome([a2,a1,a0], x)` et en exécutant symboliquement chacune des affectations.\n",
    "\n",
    "| Point | p | x | y |\n",
    "|---|---|---|---|\n",
    "| Point 1 | [a2,a1,a0] | x | 0 |\n",
    "| Point 2 | [a2,a1,a0] | x | a2 |\n",
    "| Point 2 | [a2,a1,a0] | x | a2 * x +  a1 |\n",
    "| Point 2 | [a2,a1,a0] | x |a2 x**2 + a1 x + a0|\n",
    "\n",
    "Dans ce cas l'interprète Python ne peut vérifier le résultat, mais cela permet de raisonner sur le programme et d'obtenir une formulation qui correspond bien à ce que l'on souhaitait calculer.\n",
    "\n",
    "**Attention** : l'évaluation symbolique est à manier avec précaution, car la valeur attribuée à chaque variable, peut changer à chaque affectation. On ne peut substituer à une variable que sa valeur à un moment donné de l'exécution.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* On peut **évaluer** le temps d'exécution d'un programme où le nombre d'affectations ou de tests exécutés."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tri(t):\n",
    "    for i in range(len(t) - 1):\n",
    "        for j in range(i + 1, len(t)):\n",
    "            if t[j] < t[i]:\n",
    "                t[i], t[j] = t[j], t[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tab = [2, 8, 6, 12, 4, 9]\n",
    "tri(Tab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Activité** : on peut évaluer à 15 le nombre de tests et à 5 le nombre d'affectations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cette évaluation peut se faire pour une donnée concrète : on peut alors instrumenter le programme pour lui faire faire cette évaluation en y ajoutant des compteurs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tri(t):\n",
    "    nb_tests = 0\n",
    "    nb_affectations = 0\n",
    "    for i in range(len(t) - 1):\n",
    "        for j in range(i+1, len(t)):\n",
    "            nb_tests += 1\n",
    "            if t[j] < t[i]:\n",
    "                nb_affectations += 1\n",
    "                t[i], t[j] = t[j], t[i]\n",
    "    return nb_tests, nb_affectations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tab = [2, 8, 6, 12, 4, 9]\n",
    "tri(Tab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut aussi vouloir faire une évaluation symbolique en fonction d'un paramètre qui caractèrise la donnée : sa taille par exemple. Dans l'exemple, si $n$ est la taille du tableau `t` le nombre de tests est toujours égal à $\\sum_{i=1}^{n-1} i$, soit $n (n-1) / 2$. C'est le domaine de l'étude de la complexité des algorithmes..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "De manière générale, **évaluer** un programme consiste donc à regarder ce programme comme une donnée et à en calculer une valeur par une méthode particulière. Ce changement de plan du programmeur consiste à regarder son programme tel qu'il est — et non tel qu'il aurait voulu qu'il soit.\n",
    "\n",
    "La compétence *évaluer* est fondamentale pour mettre au point un programme.\n",
    "\n",
    "**Remarque** : D'autres mécanismes d'évaluation existent dans d'autres langages mais n'existent pas en Python : l'évaluation partielle des fonctions (en Caml par exemple) et l'évaluation retardée dans les langages paresseux (voir Haskell par exemple)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Equipe pédagoqique DIU EIL, ressource éducative libre distribuée sous [Licence Creative Commons Attribution - Pas d’Utilisation Commerciale - Partage dans les Mêmes Conditions 4.0 International](http://creativecommons.org/licenses/by-nc-sa/4.0/) ![Licence Creative Commons](https://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
